# ğŸ–¼ï¸ Image Captioning using Deep Learning

This project implements an **automatic Image Captioning system** that generates meaningful textual descriptions for images using Deep Learning techniques.

The model is trained on the **Flickr30k dataset**, which contains thousands of images paired with multiple human-written captions, making it ideal for learning visualâ€“language relationships.

The architecture combines:

- **CNN** for image feature extraction  
- **LSTM** for sequence modeling and caption generation  

---

## ğŸ“Œ Project Files

- `Image_captioning.ipynb` â†’ Main implementation notebook  
- `Image_captioning_report.pdf` â†’ Detailed technical report with methodology and results  

---

## ğŸ“‚ Dataset Used

### ğŸ“¸ Flickr30k Dataset

- Contains ~31,000 real-world images
- Each image is annotated with 5 captions
- Widely used for benchmarking image captioning models

Dataset is used for:
- Training the caption generator
- Vocabulary building
- Model evaluation

---

## ğŸš€ Features

- Image preprocessing and feature extraction
- Tokenization and vocabulary creation
- Caption sequence generation
- CNN-LSTM architecture
- Model training and validation
- Caption prediction on new images

---

## ğŸ§  Tech Stack

- Python
- TensorFlow / Keras
- NumPy
- Pandas
- Matplotlib
- OpenCV
- NLTK
- tqdm

---


